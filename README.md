
# TD1
#### Qizheng WANG
#### Remarque: J'ai termin√© les deux premi√®res parties, **README** est l'enregistrement de mes r√©sultats.
# I. Partie 1

`pandoc -s --toc README.md --css=./github-pandoc.css -o README.html`


## Produit matrice-matrice

### Effet de la taille de la matrice

  n            | MFlops
---------------|--------
1024 (origine) | 61.6447
1023           | 96.6385
1025           | 96.7703


*Expliquer les r√©sultatsÔºö*

*Les r√©sultats montrent une variation significative des performances en fonction de la taille de la matrice. Pour ùëõ=1024 n=1024, la performance est plus faible (61.64 MFlops) par rapport aux tailles non align√©es ùëõ=1023 n=1023 et ùëõ=1025
n=1025 (~96.7 MFlops). Cette diff√©rence est due √† l‚Äôalignement m√©moire : 1024 est une puissance de 2, ce qui peut entra√Æner des conflits de cache (cache associatif), tandis que 1023 et 1025 r√©partissent mieux les acc√®s m√©moire, r√©duisant les cache misses et am√©liorant ainsi la performance.*

### Permutation des boucles

*Expliquer comment est compil√© le code (ligne de make ou de gcc) : on aura besoin de savoir l'optim, les param√®tres, etc. Par exemple :*

`make TestProduct.exe && ./TestProduct.exe 1024`


  ordre           | time    | MFlops  |
------------------|---------|---------|
i,j,k (origine)   | 16.0961 | 133.417 |
j,i,k             | 23.5417 | 91.2202 |
i,k,j             | 39.2870 | 54.6614 |
k,i,j             | 28.3425 | 75.7691 |
j,k,i             | 12.0201 | 178.658 |
k,j,i             | 13.5173 | 158.869 |


*Discuter les r√©sultats.*

Les r√©sultats montrent une forte d√©pendance de la performance √† l'ordre des boucles. L'ordre j,k,i est le plus rapide (12.02s, 178.66 MFlops), suivi de k,j,i (13.52s, 158.87 MFlops), indiquant une meilleure exploitation du cache et de la localit√© m√©moire. Ces r√©sultats sugg√®rent que prioriser l'it√©ration sur les colonnes (j) ou les lignes (k) avant les multiplications internes am√©liore les performances.



### OMP sur la meilleure boucle

`make TestProduct.exe && OMP_NUM_THREADS=8 ./TestProduct.exe 1024`

  OMP_NUM         | MFlops  |  MFlops(n=2048) | MFlops(n=512)  | 
------------------|---------|-----------------|----------------|
1                 | 17.8758 |     17.8758     |     36.5950    |
2                 | 34.0799 |     19.6172     |     49.5074    |
3                 | 51.9702 |     19.6053     |     53.0270    |
4                 | 51.9944 |     19.3541     |     87.4854    |
5                 | 55.4674 |     19.2349     |     95.6910    |
6                 | 54.6610 |     22.0020     |     88.8524    |
7                 | 58.7588 |     51.5638     |     109.757    |
8                 | 63.2577 |     54.3109     |     81.0722    |

*Tracer les courbes de speedup (pour chaque valeur de n), discuter les r√©sultats.*
![Description](./output.png)

Les courbes de speedup montrent une acc√©l√©ration variable en fonction de la taille de la matrice et du nombre de threads. Pour \( n=1024 \) et \( n=512 \), le speedup est presque lin√©aire jusqu'√† 8 threads, indiquant une bonne scalabilit√©. Cependant, pour \( n=2048 \), l'acc√©l√©ration reste faible jusqu'√† 6 threads, puis augmente fortement √† 7 et 8 threads. Cela peut √™tre d√ª √† la saturation de la bande passante m√©moire et √† une mauvaise exploitation du cache pour de grandes matrices.


### Produit par blocs

`make TestProduct.exe && ./TestProduct.exe 1024`

  szBlock         | MFlops  | MFlops(n=2048) | MFlops(n=512)  | 
------------------|---------|----------------|----------------|
origine (=max)    | 133.417 | 138.241 | 159.076 |
32                | 151.514 | 146.692 | 171.390 |
64                | 156.570 | 151.630 | 163.212 | 
128               | 159.440 | 157.445 | 167.792 |
256               | 150.700 | 148.322 | 153.776 |
512               | 110.821 | 109.987 | 139.625 |
1024              | 51.7021 | 52.4546 | 80.7186 |

*Discuter les r√©sultats.*
L‚Äôoptimisation par blocs am√©liore la performance du produit matrice-matrice en exploitant mieux la hi√©rarchie de la m√©moire cache. On observe que la performance (en MFlops) augmente avec la taille du bloc jusqu‚Äô√† un certain seuil, avec un maximum autour de 128. Au-del√† de cette valeur, les performances diminuent, notamment pour `szBlock = 512` et `1024`, car les blocs deviennent trop grands pour tenir dans le cache L1 ou L2.


### Bloc + OMP


  szBlock      | OMP_NUM | MFlops  | MFlops(n=2048) | MFlops(n=512)  |
---------------|---------|---------|----------------|----------------|
1024           |  1      | 58.7836 |    34.3739     |    97.9819     |
1024           |  8      | 61.8933 |    43.2985     |    133.653     |
512            |  1      | 130.785 |    120.942     |    137.855     |
512            |  8      | 133.325 |    122.377     |    123.671     |

*Discuter les r√©sultats.*

L‚Äôoptimisation par blocs am√©liore les performances tant que la taille des blocs reste adapt√©e √† la capacit√© du cache. Un szBlock trop grand r√©duit les performances √† cause des cache misses et du mauvais √©quilibrage du travail entre threads. Dans ce cas, szBlock = 512 semble √™tre le meilleur compromis pour tirer profit √† la fois du cache et du parall√©lisme.

### Comparaison avec BLAS, Eigen et numpy

*Comparer les performances avec un calcul similaire utilisant les biblioth√®ques d'alg√®bre lin√©aire BLAS, Eigen et/ou numpy.*

| Biblioth√®ques  | MFlops  | MFlops(n=2048) |
|      BLAS      | 39426.9 |    35753.1     |

Les performances du produit matrice-matrice avec BLAS sont plusieurs ordres de grandeur sup√©rieures √† celles de notre impl√©mentation optimis√©e. Parce que BLAS a exploite des optimisations bas niveau.

### Tips

```
	env
	OMP_NUM_THREADS=4 ./produitMatriceMatrice.exe
```

```
    $ for i in $(seq 1 4); do elap=$(OMP_NUM_THREADS=$i ./TestProductOmp.exe|grep "Temps CPU"|cut -d " " -f 7); echo -e "$i\t$elap"; done > timers.out
```




# II. Partie 2 - Jeton
## II.1 R√©sultats des temps d'ex√©cution MPI

### Tableau des temps d'ex√©cution

| Nombre de processus (p) | Temps d'ex√©cution (s) |
|------------------------|----------------------|
| 1                      | 0.003123             |
| 2                      | 0.004135             |
| 4                      | 0.042656             |
| 8                      | 0.223999             |

---

### Calcul du Speedup

Le Speedup est d√©fini par la formule :

\[
S(p) = \frac{T_1}{T_p}
\]

| Nombre de processus (p) | Temps d'ex√©cution (s) | Speedup \( S(p) \) |
|------------------------|----------------------|-----------------|
| 1                      | 0.003123             | 1.00            |
| 2                      | 0.004135             | 1.51            |
| 4                      | 0.042656             | 0.29            |
| 8                      | 0.223999             | 0.11            |

---

### Analyse des r√©sultats

- On observe que le temps d'ex√©cution n'est pas lin√©airement r√©duit avec l'augmentation du nombre de processus.
- Le Speedup est bien inf√©rieur √† l'id√©al (qui serait proche de p). Cela indique probablement que la communication MPI domine le temps de calcul, rendant l'ex√©cution inefficace au-del√† de 4 processus.






## II.2 R√©sultats des temps d'ex√©cution OpenMP

### Tableau des temps d'ex√©cution OpenMP

| Nombre de threads (p) | Temps d'ex√©cution (s) |
|----------------------|----------------------|
| 1                    | 0.000542             |
| 2                    | 0.000844             |
| 4                    | 0.000848             |
| 8                    | 0.002269             |

---

### Calcul du Speedup OpenMP

Le Speedup est d√©fini par la formule :

\[
S(p) = \frac{T_1}{T_p}
\]

| Nombre de threads (p) | Temps d'ex√©cution (s) | Speedup \( S(p) \) |
|----------------------|----------------------|-----------------|
| 1                    | 0.000542             | 1.00            |
| 2                    | 0.000844             | 1.28            |
| 4                    | 0.000848             | 2.55            |
| 8                    | 0.002269             | 1.92            |

---

### Analyse des r√©sultats OpenMP

Le Speedup attendu en OpenMP est g√©n√©ralement lin√©aire avec le nombre de threads, mais ici, on observe une acc√©l√©ration tr√®s faible, ce qui indique probablement une surcharge due √† la gestion des threads ou un manque d'optimisation des acc√®s m√©moire.



## II.3 R√©sultats des temps d'ex√©cution MPI (Python)
### Tableau des temps d'ex√©cution MPI (Python)

| Nombre de processus (p) | Temps d'ex√©cution (s) |
|------------------------|----------------------|
| 1                      | 0.006500             |
| 2                      | 0.023315             |
| 4                      | 0.129870             |
| 8                      | 0.463661             |

---

### Calcul du Speedup MPI (Python)

\[
S(p) = \frac{T_2}{T_p}
\]

| Nombre de processus (p) | Temps d'ex√©cution (s) | Speedup \( S(p) \) |
|------------------------|----------------------|-----------------|
| 1                      | 0.016500             | 1.00            |
| 2                      | 0.034021             | 0.97            |
| 4                      | 0.068253             | 0.96            |
| 8                      | 0.148798             | 0.88            |

---

### Analyse des r√©sultats MPI (Python)

Contrairement √† ce qu'on pourrait attendre, l'augmentation du nombre de processus entra√Æne une d√©gradation des performances.













# III. Partie 2 - Calcul de Pi

## III.1 Tableau des temps d'ex√©cution

### M√©thode S√©quentielle
| Nombre de processus/threads | Temps d'ex√©cution (s) |
|----------------------------|----------------------|
| 1 (S√©quentiel)             | 2.596822             |

### OpenMP (M√©moire partag√©e)
| Nombre de threads | Temps d'ex√©cution (s) |
|------------------|----------------------|
| 1               | 1.037327              |
| 2               | 0.569004              |
| 4               | 0.586210              |
| 8               | 0.633730              |

### MPI (M√©moire distribu√©e en C)
| Nombre de processus | Temps d'ex√©cution (s) |
|--------------------|----------------------|
| 1                | 1.060543              |
| 2                | 0.560285              |
| 4                | 0.541664              |
| 8                | 0.529933              |

### MPI (Python - `mpi4py`)
| Nombre de processus | Temps d'ex√©cution (s) |
|--------------------|----------------------|
| 1                | 47.035593             |
| 2                | 26.989896             |
| 4                | 27.864133             |
| 8                | 28.408249             |

---

## III.2 Tableau du Speedup

Le speedup est calcul√© comme :
\[
S(p) = \frac{T_1}{T_p}
\]

### OpenMP
| Nombre de threads | Temps d'ex√©cution (s) | Speedup \( S(p) \) |
|------------------|----------------------|-----------------|
| 1               | 1.037327              | 1.00            |
| 2               | 0.569004              | 1.82            |
| 4               | 0.586210              | 1.77            |
| 8               | 0.633730              | 1.64            |

### MPI en C
| Nombre de processus | Temps d'ex√©cution (s) | Speedup \( S(p) \) |
|--------------------|----------------------|-----------------|
| 1                | 1.060543              | 1.00            |
| 2                | 0.560285              | 1.89            |
| 4                | 0.541664              | 1.96            |
| 8                | 0.529933              | 2.00            |

### MPI en Python
| Nombre de processus | Temps d'ex√©cution (s) | Speedup \( S(p) \) |
|--------------------|----------------------|-----------------|
| 1                | 47.035593             | 1.00            |
| 2                | 26.989896             | 1.74            |
| 4                | 27.864133             | 1.69            |
| 8                | 28.408249             | 1.66            |

---

## III.3 Analyse des r√©sultats

### 1. OpenMP (M√©moire partag√©e)
- Le speedup est presque lin√©aire pour 2 threads (1.82x).
- √Ä partir de 4 threads, l'acc√©l√©ration se stabilise voire diminue l√©g√®rement, indiquant une saturation m√©moire ou une surcharge due √† la synchronisation des threads.

### 2. MPI en C (M√©moire distribu√©e)
- Tr√®s bon speedup jusqu'√† 4 processus (~1.96x).

### 3. MPI en Python (`mpi4py`)
- Performances globalement beaucoup plus lentes que C et OpenMP.
- L‚Äôacc√©l√©ration existe (1.74x √† 2 processus) mais s'effondre apr√®s 4 processus.

Conclusion : Pour un calcul parall√®le performant, OpenMP est efficace pour des syst√®mes mono-machine, tandis que MPI en C est pr√©f√©rable pour des ex√©cutions distribu√©es.

